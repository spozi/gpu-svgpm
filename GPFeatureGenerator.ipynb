{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPFeatureGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMA0zvQuwmJNn92WYe5ies9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spozi/gpu-svgpm/blob/main/GPFeatureGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_ZEI1ZknHnT",
        "outputId": "a9d78e23-8af9-42ac-b016-8a1a136aceda"
      },
      "source": [
        "!pip install -U deap imbalanced-learn scikit-learn-intelex"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deap\n",
            "  Downloading deap-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 20 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 30 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 81 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 92 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 160 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.4.3)\n",
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.8.0-py3-none-any.whl (206 kB)\n",
            "\u001b[K     |████████████████████████████████| 206 kB 9.9 MB/s \n",
            "\u001b[?25hCollecting scikit-learn-intelex\n",
            "  Downloading scikit_learn_intelex-2021.3.0-py37-none-manylinux1_x86_64.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deap) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Collecting scikit-learn>=0.24\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 33 kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Collecting daal4py==2021.3.0\n",
            "  Downloading daal4py-2021.3.0-py37-none-manylinux1_x86_64.whl (15.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.9 MB 59 kB/s \n",
            "\u001b[?25hCollecting daal==2021.3.0\n",
            "  Downloading daal-2021.3.0-py2.py3-none-manylinux1_x86_64.whl (292.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 292.9 MB 44 kB/s \n",
            "\u001b[?25hCollecting tbb==2021.*\n",
            "  Downloading tbb-2021.3.0-py2.py3-none-manylinux1_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 50.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: tbb, threadpoolctl, daal, scikit-learn, daal4py, scikit-learn-intelex, imbalanced-learn, deap\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.4.3\n",
            "    Uninstalling imbalanced-learn-0.4.3:\n",
            "      Successfully uninstalled imbalanced-learn-0.4.3\n",
            "Successfully installed daal-2021.3.0 daal4py-2021.3.0 deap-1.3.1 imbalanced-learn-0.8.0 scikit-learn-0.24.2 scikit-learn-intelex-2021.3.0 tbb-2021.3.0 threadpoolctl-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irgcpLpFBF-3",
        "outputId": "0dcbf871-be9a-4967-e12c-c767ad715857"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import site\n",
        "sys.path.append(os.path.join(os.path.dirname(site.getsitepackages()[0]), \"site-packages\"))\n",
        "\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASNSiD1zsUJ8"
      },
      "source": [
        "#@title SVGPM Configuration\n",
        "#@markdown ---\n",
        "#@markdown ### Enter a file path:\n",
        "train_file_path = \"/content/WormsTwoClass_train.csv\" #@param {type:\"string\"}\n",
        "test_file_path = \"/content/WormsTwoClass_test.csv\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Enter SVGPM Parameters:\n",
        "population_size = 500 #@param {type:\"slider\", min:0, max:1000, step:2}\n",
        "number_of_generation = 500 #@param {type:\"slider\", min:0, max:1000, step:2}\n",
        "#@markdown ---"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjaiCtAhnPHP"
      },
      "source": [
        "from collections import Counter\n",
        "# from imblearn.datasets import fetch_datasets\n",
        "# ecoli = fetch_datasets()['ecoli']\n",
        "# ecoli.data.shape\n",
        "\n",
        "# X = ecoli.data\n",
        "# y = ecoli.target\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "#Train and test file\n",
        "train_file = train_file_path\n",
        "test_file = test_file_path\n",
        "\n",
        "train_data = np.loadtxt(train_file, delimiter=\",\", skiprows=1)\n",
        "test_data = np.loadtxt(test_file, delimiter=\",\", skiprows=1)\n",
        "\n",
        "X_train = train_data[:, 1:]\n",
        "X_test = test_data[:, 1:]\n",
        "\n",
        "#Normalize each feature to 0 and 1\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#y_train\n",
        "y_train = train_data[:, 0].astype(int)\n",
        "y_test = test_data[:, 0].astype(int)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkDFiNBCDHCD",
        "outputId": "e612f6c0-ed05-4e89-8645-60961d53455b"
      },
      "source": [
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(181, 900) (77, 900)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPTXrbRK9CbG"
      },
      "source": [
        "#Specify X_train, X_test, y_train, y_test here\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "# X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n4G13-SRMp2"
      },
      "source": [
        "Version 2 GP Feature Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UpoPcmQMMda"
      },
      "source": [
        "import random\n",
        "import operator\n",
        "import math\n",
        "import statistics\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from deap import algorithms\n",
        "from deap import base\n",
        "from deap import creator\n",
        "from deap import tools\n",
        "from deap import gp\n",
        "\n",
        "# Define new functions\n",
        "def protectedDiv(left, right):\n",
        "    try:\n",
        "        return left / right\n",
        "    except ZeroDivisionError:\n",
        "        return 1\n",
        "\n",
        "nFeatures = X_train.data.shape[1]\n",
        "pset = gp.PrimitiveSet(\"MAIN\", nFeatures) \n",
        "pset.addPrimitive(operator.add, 2)\n",
        "pset.addPrimitive(operator.sub, 2)\n",
        "pset.addPrimitive(operator.mul, 2)\n",
        "pset.addPrimitive(protectedDiv, 2)\n",
        "pset.addPrimitive(operator.neg, 1)\n",
        "pset.addPrimitive(math.erfc, 1)\n",
        "pset.addPrimitive(math.erf, 1)\n",
        "pset.addPrimitive(math.exp, 1)\n",
        "pset.addPrimitive(math.gamma, 1)\n",
        "pset.addPrimitive(math.sqrt, 1)\n",
        "pset.addPrimitive(math.cos, 1)\n",
        "pset.addPrimitive(math.sin, 1)\n",
        "pset.addEphemeralConstant(\"rand\", lambda: round(random.uniform(0.1, 1.0), 10))\n",
        "\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Tree\", gp.PrimitiveTree)\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"main_expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=5)\n",
        "toolbox.register('MAIN', tools.initIterate, creator.Tree, toolbox.main_expr)\n",
        "\n",
        "func_cycle = [toolbox.MAIN]\n",
        "\n",
        "toolbox.register(\"individual\", tools.initCycle, creator.Individual, func_cycle)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71GGjz4mQ9SS"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.feature_selection import SelectKBest, chi2, SelectPercentile\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def evalSymbReg(score):\n",
        "  return score,\n",
        "\n",
        "def evalSymbRegPop(population):\n",
        "  # Evaluate each individual in population\n",
        "  #1. Compute the expression of every individual\n",
        "  list_vecs = []\n",
        "  for individual in population:\n",
        "    #The following code should be optimized/vectorized\n",
        "    #Evaluating expression on each vector\n",
        "    func = toolbox.compile(expr=individual)\n",
        "    vec = []\n",
        "    for x in X_train: #Iterate every vector x (row) in data (matrix) X\n",
        "      try:\n",
        "        val = func(*x)\n",
        "        vec.append(val)\n",
        "      except:\n",
        "        vec.append(0)\n",
        "    list_vecs.append(vec)\n",
        "\n",
        "  #2. Convert list_vecs to numpy array\n",
        "  evaluated_X = np.array(list_vecs).T\n",
        "  evaluated_X = np.float32(evaluated_X)\n",
        "  evaluated_X = np.nan_to_num(evaluated_X, copy=True, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "  #3. Individual (feature) selection\n",
        "  # https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-using-selectfrommodel\n",
        "  # print(\"Feature selection\")\n",
        "  # print(evaluated_X.dtype)\n",
        "  clf = ExtraTreesClassifier(n_estimators=50)\n",
        "  clf = clf.fit(evaluated_X, y_train)\n",
        "\n",
        "  #4. Extract features that at top threshold (get the 75 percentile)\n",
        "  # print(\"Feature extraction\")\n",
        "  q1 = np.percentile(clf.feature_importances_, 50)  #Get the top 75 percentile features\n",
        "  features = [True if val >= q1 else False for val in clf.feature_importances_.tolist()] #Get the features indices\n",
        "  X_train_new = evaluated_X[:, features]  #Extract the required data\n",
        "\n",
        "  #5. Merge X_train with X_train_new row-wise\n",
        "  X_train_new = np.hstack((X_train, X_train_new))\n",
        "\n",
        "  #6. Use svc to get total nSV\n",
        "  # print(\"SVC\")\n",
        "  clf_svc = SVC(C=1, kernel='linear') #Smaller C value will increase the margin size of the hyperplane. The new features should be generate such that it has very high linear seperability\n",
        "  clf_svc.fit(X_train_new, y_train)\n",
        "  y_pred = clf_svc.predict(X_train_new)\n",
        "\n",
        "  #7. Compute the score\n",
        "  nSV = clf_svc.support_vectors_.shape[0]\n",
        "  f1 = f1_score(y_train, y_pred)\n",
        "  fitness = f1/nSV\n",
        "\n",
        "  #8. Output the fitness value\n",
        "  ind_pop_fitness = []\n",
        "  for f in features:\n",
        "    if f is True:\n",
        "      ind_pop_fitness.append(fitness) #If the feature is in the tree, set the fitness value to fitness\n",
        "    else:\n",
        "      ind_pop_fitness.append(0)       #If the feature is in the tree, set the fitness value to 0\n",
        "  \n",
        "  return ind_pop_fitness\n",
        "\n",
        "psets = [pset]\n",
        "toolbox.register(\"compile\", gp.compileADF, psets=psets)\n",
        "toolbox.register('evaluate', evalSymbReg)\n",
        "toolbox.register('select', tools.selTournament, tournsize=3)\n",
        "toolbox.register('mate', gp.cxOnePoint)\n",
        "toolbox.register(\"expr\", gp.genFull, min_=1, max_=2)\n",
        "toolbox.register('mutate', gp.mutUniform, expr=toolbox.expr)\n",
        "\n",
        "toolbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=80))\n",
        "toolbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=80))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_7HXCBEMQmU",
        "outputId": "c8081ce1-eedf-4453-9234-fd95f26d54ca"
      },
      "source": [
        "def main():\n",
        "  random.seed(1024)\n",
        "  ind = toolbox.individual()\n",
        "  \n",
        "  pop = toolbox.population(n=population_size)\n",
        "  hof = tools.HallOfFame(population_size)\n",
        "  stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "  stats.register(\"avg\", np.mean)\n",
        "  stats.register(\"std\", np.std)\n",
        "  stats.register(\"min\", np.min)\n",
        "  stats.register(\"max\", np.max)\n",
        "\n",
        "  logbook = tools.Logbook()\n",
        "  logbook.header = \"gen\", \"evals\", \"std\", \"min\", \"avg\", \"max\"\n",
        "\n",
        "  CXPB, MUTPB, NGEN = 0.5, 0.2, number_of_generation\n",
        "\n",
        "  # # Evaluate the entire population\n",
        "#################################This is for adhoc code draft##########################################################################\n",
        "  #0. Test each indvidiual in population\n",
        "\n",
        "  # list_vecs = []\n",
        "  # for ind in pop:\n",
        "  #   func = toolbox.compile(expr=ind)\n",
        "\n",
        "  #   #The following code should be optimized\n",
        "  #   #Evaluating expression on each vector\n",
        "  #   func = toolbox.compile(expr=ind)\n",
        "  #   vec = []\n",
        "  #   for x in X: #Iterate every vector x (row) in data (matrix) X\n",
        "  #     val = func(*x)\n",
        "  #     # try:\n",
        "  #     #   val = func(*x)\n",
        "  #     # except:\n",
        "  #     #   print(x, [str(eq) for eq in ind])\n",
        "  #       # print(ind[0])\n",
        "  #     # print(val)\n",
        "  #     vec.append(val)\n",
        "  #   list_vecs.append(vec)\n",
        "\n",
        "  # #2. Convert list_vecs to numpy array\n",
        "  # evaluated_X = np.array(list_vecs).T\n",
        "  # evaluated_X = np.nan_to_num(evaluated_X, copy=True, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "  # clf = ExtraTreesClassifier(n_estimators=50)\n",
        "  # clf = clf.fit(evaluated_X, y)\n",
        "\n",
        "  # #4. Extract features that at top threshold (get the 75 percentile)\n",
        "  # q1 = np.percentile(clf.feature_importances_, 75)\n",
        "  # print( clf.feature_importances_.tolist())\n",
        "  # features = [True if val >= q1 else False for val in clf.feature_importances_.tolist()]\n",
        "  # print(len(features), evaluated_X.shape)\n",
        "  # new_X = evaluated_X[:, features]\n",
        "  # print(new_X.shape)\n",
        "  # 2/0\n",
        "\n",
        "  # #4. \n",
        "\n",
        "\n",
        "\n",
        "###########################################################################################################\n",
        "  #1. Compute the metric on the set of individuals\n",
        "  ind_pop_fitness = evalSymbRegPop(pop)\n",
        "\n",
        "  #2. Then, determine the best individual using toolbox\n",
        "  for ind, fitness in zip(pop, ind_pop_fitness):\n",
        "    ind.fitness.values = toolbox.evaluate(fitness)\n",
        "\n",
        "  hof.update(pop)\n",
        "  record = stats.compile(pop)\n",
        "  logbook.record(gen=0, evals=len(pop), **record)\n",
        "  print(logbook.stream)\n",
        "\n",
        "  for g in range(1, NGEN):\n",
        "    # Select the offspring\n",
        "    offspring = toolbox.select(pop, len(pop))\n",
        "    # Clone the offspring\n",
        "    offspring = [toolbox.clone(ind) for ind in offspring]\n",
        "\n",
        "    # Apply crossover and mutation\n",
        "    for ind1, ind2 in zip(offspring[::2], offspring[1::2]):\n",
        "        for tree1, tree2 in zip(ind1, ind2):\n",
        "            if random.random() < CXPB:\n",
        "                toolbox.mate(tree1, tree2)\n",
        "                del ind1.fitness.values\n",
        "                del ind2.fitness.values\n",
        "\n",
        "    for ind in offspring:\n",
        "        for tree, pset in zip(ind, psets):\n",
        "            if random.random() < MUTPB:\n",
        "                toolbox.mutate(individual=tree, pset=pset)\n",
        "                del ind.fitness.values\n",
        "                        \n",
        "    # Evaluate the individuals with an invalid fitness\n",
        "    invalids = [ind for ind in offspring if not ind.fitness.valid]\n",
        "\n",
        "    #1. Compute the metric on the set of individuals\n",
        "    ind_pop_invalid_fitness = evalSymbRegPop(invalids)\n",
        "\n",
        "    #2. Then, determine the best individual using toolbox\n",
        "    for ind, fitness in zip(invalids, ind_pop_invalid_fitness):\n",
        "      ind.fitness.values = toolbox.evaluate(fitness)\n",
        "            \n",
        "    # Replacement of the population by the offspring\n",
        "    pop = offspring\n",
        "    hof.update(pop)\n",
        "    record = stats.compile(pop)\n",
        "    logbook.record(gen=g, evals=len(invalids), **record)\n",
        "    print(logbook.stream)\n",
        "  \n",
        "  print('Best individual : ', hof[0][0], hof[0].fitness)\n",
        "  return pop, stats, hof\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pop, stats, hof = main()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gen\tevals\tstd        \tmin\tavg        \tmax        \n",
            "0  \t500  \t0.000158378\t0  \t0.000158378\t0.000316756\n",
            "1  \t303  \t0.00349813 \t0  \t0.00246771 \t0.00775695 \n",
            "2  \t288  \t0.00389942 \t0  \t0.00447069 \t0.00810811 \n",
            "3  \t298  \t0.00380091 \t0  \t0.00538888 \t0.00817041 \n",
            "4  \t291  \t0.00383059 \t0  \t0.00573932 \t0.00849618 \n",
            "5  \t285  \t0.00372525 \t0  \t0.00567232 \t0.00849618 \n",
            "6  \t319  \t0.00391554 \t0  \t0.00559719 \t0.00849618 \n",
            "7  \t313  \t0.00410051 \t0  \t0.00593294 \t0.00913771 \n",
            "8  \t304  \t0.00408381 \t0  \t0.00593957 \t0.00913771 \n",
            "9  \t286  \t0.00439775 \t0  \t0.00373798 \t0.00913771 \n",
            "10 \t302  \t0.00446756 \t0  \t0.00546502 \t0.00928105 \n",
            "11 \t298  \t0.00446603 \t0  \t0.00352448 \t0.00928105 \n",
            "12 \t308  \t0.00459705 \t0  \t0.00576942 \t0.00962001 \n",
            "13 \t279  \t0.00428427 \t0  \t0.006282   \t0.00962001 \n",
            "14 \t305  \t0.00425145 \t0  \t0.00617088 \t0.00962001 \n",
            "15 \t284  \t0.00442761 \t0  \t0.00682092 \t0.0102124  \n",
            "16 \t300  \t0.00418224 \t0  \t0.00590793 \t0.0102124  \n",
            "17 \t292  \t0.00467842 \t0  \t0.00380599 \t0.0102124  \n",
            "18 \t282  \t0.00468119 \t0  \t0.0060229  \t0.0102124  \n",
            "19 \t291  \t0.00442952 \t0  \t0.00670509 \t0.0102124  \n",
            "20 \t316  \t0.00448516 \t0  \t0.00639881 \t0.0102124  \n",
            "21 \t312  \t0.00442783 \t0  \t0.00645802 \t0.0102124  \n",
            "22 \t307  \t0.00450682 \t0  \t0.00661359 \t0.0102124  \n",
            "23 \t303  \t0.0206965  \t0  \t0.0189774  \t0.0496732  \n",
            "24 \t322  \t0.0201609  \t0  \t0.0151644  \t0.0496732  \n",
            "25 \t290  \t0.0213148  \t0  \t0.0139977  \t0.0496732  \n",
            "26 \t307  \t0.019207   \t0  \t0.0130747  \t0.0496732  \n",
            "27 \t292  \t0.0193284  \t0  \t0.0111479  \t0.0496732  \n",
            "28 \t293  \t0.0179728  \t0  \t0.0129295  \t0.0496732  \n",
            "29 \t298  \t0.0178148  \t0  \t0.0135095  \t0.0496732  \n",
            "30 \t301  \t0.0195681  \t0  \t0.0118443  \t0.0496732  \n",
            "31 \t283  \t0.0189453  \t0  \t0.0144896  \t0.0496732  \n",
            "32 \t321  \t0.0190747  \t0  \t0.0111147  \t0.0496732  \n",
            "33 \t296  \t0.0186234  \t0  \t0.0138331  \t0.0496732  \n",
            "34 \t290  \t0.0175364  \t0  \t0.0134286  \t0.0496732  \n",
            "35 \t290  \t0.0185338  \t0  \t0.0113488  \t0.0496732  \n",
            "36 \t283  \t0.0178712  \t0  \t0.0119066  \t0.0496732  \n",
            "37 \t290  \t0.0181334  \t0  \t0.0136967  \t0.0496732  \n",
            "38 \t297  \t0.0192555  \t0  \t0.0114823  \t0.0496732  \n",
            "39 \t300  \t0.0185099  \t0  \t0.0134758  \t0.0496732  \n",
            "40 \t296  \t0.0187598  \t0  \t0.0153674  \t0.0496732  \n",
            "41 \t309  \t0.0180365  \t0  \t0.0143479  \t0.0496732  \n",
            "42 \t287  \t0.018507   \t0  \t0.0152752  \t0.0496732  \n",
            "43 \t307  \t0.0187602  \t0  \t0.0146243  \t0.0496732  \n",
            "44 \t318  \t0.0193234  \t0  \t0.011283   \t0.0496732  \n",
            "45 \t309  \t0.0171067  \t0  \t0.0115176  \t0.0496732  \n",
            "46 \t268  \t0.0187482  \t0  \t0.0114199  \t0.0496732  \n",
            "47 \t281  \t0.01921    \t0  \t0.0143396  \t0.0496732  \n",
            "48 \t301  \t0.0187952  \t0  \t0.0145233  \t0.0496732  \n",
            "49 \t289  \t0.0187185  \t0  \t0.0146728  \t0.0496732  \n",
            "50 \t290  \t0.0180728  \t0  \t0.0134866  \t0.0496732  \n",
            "51 \t295  \t0.0176001  \t0  \t0.0127983  \t0.0496732  \n",
            "52 \t291  \t0.0179973  \t0  \t0.0133806  \t0.0496732  \n",
            "53 \t314  \t0.0181436  \t0  \t0.0133444  \t0.0496732  \n",
            "54 \t280  \t0.0188293  \t0  \t0.015008   \t0.0496732  \n",
            "55 \t298  \t0.0186122  \t0  \t0.014181   \t0.0496732  \n",
            "56 \t273  \t0.0191784  \t0  \t0.0150671  \t0.0496732  \n",
            "57 \t293  \t0.0187726  \t0  \t0.0142071  \t0.0496732  \n",
            "58 \t321  \t0.01862    \t0  \t0.0134277  \t0.0496732  \n",
            "59 \t294  \t0.0181168  \t0  \t0.0130554  \t0.0496732  \n",
            "60 \t290  \t0.0188847  \t0  \t0.0141683  \t0.0496732  \n",
            "61 \t295  \t0.0185623  \t0  \t0.0140356  \t0.0496732  \n",
            "62 \t319  \t0.0183106  \t0  \t0.00999201 \t0.0496732  \n",
            "63 \t290  \t0.0179497  \t0  \t0.0117406  \t0.0496732  \n",
            "64 \t311  \t0.016801   \t0  \t0.0117162  \t0.0496732  \n",
            "65 \t300  \t0.0163243  \t0  \t0.0117095  \t0.0496732  \n",
            "66 \t317  \t0.0160365  \t0  \t0.0111799  \t0.0496732  \n",
            "67 \t305  \t0.0167066  \t0  \t0.011914   \t0.0496732  \n",
            "68 \t322  \t0.0171379  \t0  \t0.0088418  \t0.0496732  \n",
            "69 \t308  \t0.0172616  \t0  \t0.011579   \t0.0496732  \n",
            "70 \t306  \t0.0165797  \t0  \t0.0116979  \t0.0496732  \n",
            "71 \t291  \t0.0170484  \t0  \t0.0125357  \t0.0496732  \n",
            "72 \t300  \t0.0174941  \t0  \t0.0127737  \t0.0496732  \n",
            "73 \t312  \t0.0173962  \t0  \t0.012511   \t0.0496732  \n",
            "74 \t287  \t0.0180063  \t0  \t0.0136159  \t0.0496732  \n",
            "75 \t277  \t0.0185203  \t0  \t0.0140738  \t0.0496732  \n",
            "76 \t318  \t0.0191981  \t0  \t0.0109279  \t0.0496732  \n",
            "77 \t271  \t0.0194387  \t0  \t0.0139867  \t0.0496732  \n",
            "78 \t321  \t0.017747   \t0  \t0.0128292  \t0.0496732  \n",
            "79 \t300  \t0.0163966  \t0  \t0.0115111  \t0.0496732  \n",
            "80 \t291  \t0.0169765  \t0  \t0.0120592  \t0.0496732  \n",
            "81 \t304  \t0.0169109  \t0  \t0.0119736  \t0.0496732  \n",
            "82 \t290  \t0.0167696  \t0  \t0.0119941  \t0.0496732  \n",
            "83 \t284  \t0.0177988  \t0  \t0.0130302  \t0.0496732  \n",
            "84 \t287  \t0.0182729  \t0  \t0.0137076  \t0.0496732  \n",
            "85 \t292  \t0.0182979  \t0  \t0.0136955  \t0.0496732  \n",
            "86 \t291  \t0.0179984  \t0  \t0.0131052  \t0.0496732  \n",
            "87 \t320  \t0.0174565  \t0  \t0.0123313  \t0.0496732  \n",
            "88 \t291  \t0.0181956  \t0  \t0.013663   \t0.0496732  \n",
            "89 \t305  \t0.0189666  \t0  \t0.0140031  \t0.0496732  \n",
            "90 \t300  \t0.017968   \t0  \t0.0128961  \t0.0496732  \n",
            "91 \t291  \t0.018155   \t0  \t0.0135143  \t0.0496732  \n",
            "92 \t289  \t0.0182832  \t0  \t0.0137104  \t0.0496732  \n",
            "93 \t317  \t0.0175052  \t0  \t0.0121374  \t0.0496732  \n",
            "94 \t295  \t0.0172542  \t0  \t0.0122352  \t0.0496732  \n",
            "95 \t298  \t0.0175392  \t0  \t0.00951546 \t0.0496732  \n",
            "96 \t290  \t0.0177008  \t0  \t0.0117112  \t0.0496732  \n",
            "97 \t319  \t0.0169197  \t0  \t0.0116195  \t0.0496732  \n",
            "98 \t300  \t0.016031   \t0  \t0.0110946  \t0.0496732  \n",
            "99 \t302  \t0.01748    \t0  \t0.0128208  \t0.0496732  \n",
            "100\t302  \t0.0182034  \t0  \t0.0131346  \t0.0496732  \n",
            "101\t303  \t0.018577   \t0  \t0.0109563  \t0.0496732  \n",
            "102\t320  \t0.0163925  \t0  \t0.0105211  \t0.0496732  \n",
            "103\t328  \t0.0156164  \t0  \t0.0102237  \t0.0496732  \n",
            "104\t288  \t0.0146911  \t0  \t0.00971559 \t0.0496732  \n",
            "105\t285  \t0.0156495  \t0  \t0.0111875  \t0.0496732  \n",
            "106\t313  \t0.0157758  \t0  \t0.0108377  \t0.0496732  \n",
            "107\t321  \t0.0154589  \t0  \t0.0102951  \t0.0496732  \n",
            "108\t303  \t0.0164245  \t0  \t0.0113857  \t0.0496732  \n",
            "109\t298  \t0.0165244  \t0  \t0.0113758  \t0.0496732  \n",
            "110\t267  \t0.0172992  \t0  \t0.0126073  \t0.0496732  \n",
            "111\t321  \t0.0163476  \t0  \t0.0108448  \t0.0496732  \n",
            "112\t289  \t0.0156286  \t0  \t0.0103679  \t0.0496732  \n",
            "113\t300  \t0.015019   \t0  \t0.00986968 \t0.0496732  \n",
            "114\t328  \t0.0138149  \t0  \t0.00903042 \t0.0496732  \n",
            "115\t300  \t0.01538    \t0  \t0.0105233  \t0.0496732  \n",
            "116\t309  \t0.0157806  \t0  \t0.0108153  \t0.0496732  \n",
            "117\t314  \t0.0170095  \t0  \t0.0116717  \t0.0496732  \n",
            "118\t257  \t0.0178399  \t0  \t0.0134085  \t0.0496732  \n",
            "119\t273  \t0.0188594  \t0  \t0.0144353  \t0.0496732  \n",
            "120\t314  \t0.0186628  \t0  \t0.010284   \t0.0496732  \n",
            "121\t312  \t0.0186549  \t0  \t0.0103723  \t0.0496732  \n",
            "122\t308  \t0.0182785  \t0  \t0.0122133  \t0.0496732  \n",
            "123\t290  \t0.018016   \t0  \t0.0130806  \t0.0496732  \n",
            "124\t288  \t0.0177263  \t0  \t0.0129757  \t0.0496732  \n",
            "125\t293  \t0.0187873  \t0  \t0.0141683  \t0.0496732  \n",
            "126\t296  \t0.0190343  \t0  \t0.0147649  \t0.0496732  \n",
            "127\t315  \t0.0180854  \t0  \t0.0131938  \t0.0496732  \n",
            "128\t275  \t0.0196713  \t0  \t0.0121965  \t0.0496732  \n",
            "129\t314  \t0.018159   \t0  \t0.0122925  \t0.0496732  \n",
            "130\t303  \t0.0183374  \t0  \t0.00990668 \t0.0496732  \n",
            "131\t305  \t0.017304   \t0  \t0.0114213  \t0.0496732  \n",
            "132\t308  \t0.0166902  \t0  \t0.01132    \t0.0496732  \n",
            "133\t296  \t0.0167108  \t0  \t0.0118621  \t0.0496732  \n",
            "134\t305  \t0.01585    \t0  \t0.0109667  \t0.0496732  \n",
            "135\t280  \t0.0165597  \t0  \t0.0119102  \t0.0496732  \n",
            "136\t302  \t0.0170767  \t0  \t0.0121048  \t0.0496732  \n",
            "137\t302  \t0.0169453  \t0  \t0.0118497  \t0.0496732  \n",
            "138\t296  \t0.0167988  \t0  \t0.0119276  \t0.0496732  \n",
            "139\t290  \t0.0165012  \t0  \t0.0118307  \t0.0496732  \n",
            "140\t304  \t0.0158056  \t0  \t0.0111561  \t0.0496732  \n",
            "141\t308  \t0.0154536  \t0  \t0.010774   \t0.0496732  \n",
            "142\t276  \t0.0161953  \t0  \t0.0116871  \t0.0496732  \n",
            "143\t291  \t0.0166364  \t0  \t0.0120756  \t0.0496732  \n",
            "144\t323  \t0.0182902  \t0  \t0.00975412 \t0.0496732  \n",
            "145\t297  \t0.0171388  \t0  \t0.0108288  \t0.0496732  \n",
            "146\t327  \t0.0165705  \t0  \t0.0109969  \t0.0496732  \n",
            "147\t296  \t0.017686   \t0  \t0.0125617  \t0.0496732  \n",
            "148\t300  \t0.0169177  \t0  \t0.00875856 \t0.0496732  \n",
            "149\t323  \t0.0154921  \t0  \t0.00957029 \t0.0496732  \n",
            "150\t306  \t0.0168494  \t0  \t0.00860359 \t0.0496732  \n",
            "151\t270  \t0.0175585  \t0  \t0.0108531  \t0.0496732  \n",
            "152\t298  \t0.0170633  \t0  \t0.011981   \t0.0496732  \n",
            "153\t301  \t0.0174784  \t0  \t0.0094316  \t0.0496732  \n",
            "154\t285  \t0.017575   \t0  \t0.0118969  \t0.0496732  \n",
            "155\t295  \t0.0168904  \t0  \t0.012106   \t0.0496732  \n",
            "156\t299  \t0.0168445  \t0  \t0.0121825  \t0.0496732  \n",
            "157\t285  \t0.0166254  \t0  \t0.0120817  \t0.0496732  \n",
            "158\t288  \t0.0177109  \t0  \t0.0127307  \t0.0496732  \n",
            "159\t324  \t0.0176964  \t0  \t0.0125514  \t0.0496732  \n",
            "160\t300  \t0.0175647  \t0  \t0.0128994  \t0.0496732  \n",
            "161\t303  \t0.0179073  \t0  \t0.0130772  \t0.0496732  \n",
            "162\t306  \t0.0174133  \t0  \t0.012397   \t0.0496732  \n",
            "163\t308  \t0.0165524  \t0  \t0.0112973  \t0.0496732  \n",
            "164\t296  \t0.0168096  \t0  \t0.0115397  \t0.0496732  \n",
            "165\t321  \t0.0158627  \t0  \t0.00959372 \t0.0496732  \n",
            "166\t297  \t0.0159934  \t0  \t0.0104349  \t0.0496732  \n",
            "167\t298  \t0.0161674  \t0  \t0.0110061  \t0.0496732  \n",
            "168\t303  \t0.016363   \t0  \t0.0116146  \t0.0496732  \n",
            "169\t286  \t0.0181866  \t0  \t0.0139655  \t0.0496732  \n",
            "170\t306  \t0.017929   \t0  \t0.0130607  \t0.0496732  \n",
            "171\t286  \t0.0182656  \t0  \t0.0134711  \t0.0496732  \n",
            "172\t309  \t0.0186335  \t0  \t0.0138868  \t0.0496732  \n",
            "173\t299  \t0.0187606  \t0  \t0.0130176  \t0.0496732  \n",
            "174\t277  \t0.0182129  \t0  \t0.0136175  \t0.0496732  \n",
            "175\t272  \t0.0190424  \t0  \t0.0149459  \t0.0496732  \n",
            "176\t286  \t0.0192663  \t0  \t0.015327   \t0.0496732  \n",
            "177\t263  \t0.0202283  \t0  \t0.0158679  \t0.0496732  \n",
            "178\t304  \t0.0193753  \t0  \t0.0149336  \t0.0496732  \n",
            "179\t316  \t0.0195199  \t0  \t0.0112659  \t0.0496732  \n",
            "180\t325  \t0.0177808  \t0  \t0.011986   \t0.0496732  \n",
            "181\t283  \t0.0183715  \t0  \t0.0137829  \t0.0496732  \n",
            "182\t294  \t0.018441   \t0  \t0.014116   \t0.0496732  \n",
            "183\t295  \t0.0186839  \t0  \t0.0111484  \t0.0496732  \n",
            "184\t253  \t0.0191593  \t0  \t0.013669   \t0.0496732  \n",
            "185\t279  \t0.0190631  \t0  \t0.0147199  \t0.0496732  \n",
            "186\t313  \t0.0184171  \t0  \t0.0100555  \t0.0496732  \n",
            "187\t324  \t0.0168423  \t0  \t0.0108939  \t0.0496732  \n",
            "188\t312  \t0.0158647  \t0  \t0.0106963  \t0.0496732  \n",
            "189\t291  \t0.0161978  \t0  \t0.0113161  \t0.0496732  \n",
            "190\t274  \t0.0180441  \t0  \t0.0107222  \t0.0496732  \n",
            "191\t279  \t0.0175384  \t0  \t0.0118649  \t0.0496732  \n",
            "192\t319  \t0.0164635  \t0  \t0.0113189  \t0.0496732  \n",
            "193\t318  \t0.016034   \t0  \t0.0108293  \t0.0496732  \n",
            "194\t298  \t0.0170685  \t0  \t0.00910004 \t0.0496732  \n",
            "195\t293  \t0.0172777  \t0  \t0.0116903  \t0.0496732  \n",
            "196\t307  \t0.0173896  \t0  \t0.0121807  \t0.0496732  \n",
            "197\t286  \t0.0173771  \t0  \t0.0124336  \t0.0496732  \n",
            "198\t286  \t0.0177274  \t0  \t0.012938   \t0.0496732  \n",
            "199\t297  \t0.0178116  \t0  \t0.0127311  \t0.0496732  \n",
            "200\t295  \t0.0185194  \t0  \t0.0135904  \t0.0496732  \n",
            "201\t329  \t0.0188333  \t0  \t0.0100935  \t0.0496732  \n",
            "202\t308  \t0.0182843  \t0  \t0.0119882  \t0.0496732  \n",
            "203\t321  \t0.0187968  \t0  \t0.0101868  \t0.0496732  \n",
            "204\t321  \t0.0175024  \t0  \t0.0107582  \t0.0496732  \n",
            "205\t308  \t0.0163083  \t0  \t0.0106522  \t0.0496732  \n",
            "206\t307  \t0.0163362  \t0  \t0.0100433  \t0.0496732  \n",
            "207\t288  \t0.0160807  \t0  \t0.0109541  \t0.0496732  \n",
            "208\t320  \t0.0167838  \t0  \t0.00839554 \t0.0496732  \n",
            "209\t281  \t0.0171429  \t0  \t0.0114916  \t0.0496732  \n",
            "210\t298  \t0.0168862  \t0  \t0.0120594  \t0.0496732  \n",
            "211\t338  \t0.0148123  \t0  \t0.0100197  \t0.0496732  \n",
            "212\t292  \t0.0159863  \t0  \t0.0113226  \t0.0496732  \n",
            "213\t287  \t0.0166701  \t0  \t0.012015   \t0.0496732  \n",
            "214\t301  \t0.0162637  \t0  \t0.0111486  \t0.0496732  \n",
            "215\t305  \t0.0174371  \t0  \t0.00940959 \t0.0496732  \n",
            "216\t294  \t0.0175384  \t0  \t0.0123855  \t0.0496732  \n",
            "217\t306  \t0.0165042  \t0  \t0.0119881  \t0.0496732  \n",
            "218\t315  \t0.0159464  \t0  \t0.0111477  \t0.0496732  \n",
            "219\t305  \t0.0156603  \t0  \t0.0108459  \t0.0496732  \n",
            "220\t273  \t0.0167047  \t0  \t0.0118423  \t0.0496732  \n",
            "221\t296  \t0.0167145  \t0  \t0.0118929  \t0.0496732  \n",
            "222\t312  \t0.0162076  \t0  \t0.0113743  \t0.0496732  \n",
            "223\t295  \t0.0175166  \t0  \t0.00964084 \t0.0496732  \n",
            "224\t291  \t0.0170091  \t0  \t0.0111747  \t0.0496732  \n",
            "225\t290  \t0.0179506  \t0  \t0.0102512  \t0.0496732  \n",
            "226\t292  \t0.0180473  \t0  \t0.0122614  \t0.0496732  \n",
            "227\t273  \t0.0184583  \t0  \t0.0139679  \t0.0496732  \n",
            "228\t303  \t0.018321   \t0  \t0.0136047  \t0.0496732  \n",
            "229\t307  \t0.0171999  \t0  \t0.0124145  \t0.0496732  \n",
            "230\t332  \t0.0165561  \t0  \t0.0114711  \t0.0496732  \n",
            "231\t329  \t0.016478   \t0  \t0.0112849  \t0.0496732  \n",
            "232\t303  \t0.0157402  \t0  \t0.0109588  \t0.0496732  \n",
            "233\t302  \t0.0168399  \t0  \t0.0118632  \t0.0496732  \n",
            "234\t301  \t0.0158096  \t0  \t0.0110703  \t0.0496732  \n",
            "235\t305  \t0.016534   \t0  \t0.01138    \t0.0496732  \n",
            "236\t298  \t0.0172225  \t0  \t0.00930548 \t0.0496732  \n",
            "237\t305  \t0.0168267  \t0  \t0.0110878  \t0.0496732  \n",
            "238\t304  \t0.017049   \t0  \t0.0124303  \t0.0496732  \n",
            "239\t296  \t0.017588   \t0  \t0.0131829  \t0.0496732  \n",
            "240\t311  \t0.0178753  \t0  \t0.013575   \t0.0496732  \n",
            "241\t280  \t0.0191447  \t0  \t0.014092   \t0.0496732  \n",
            "242\t262  \t0.0206135  \t0  \t0.013784   \t0.0496732  \n",
            "243\t293  \t0.019172   \t0  \t0.0138816  \t0.0496732  \n",
            "244\t298  \t0.0188966  \t0  \t0.0142236  \t0.0496732  \n",
            "245\t297  \t0.0193264  \t0  \t0.0152637  \t0.0496732  \n",
            "246\t333  \t0.0180719  \t0  \t0.0130347  \t0.0496732  \n",
            "247\t302  \t0.0175315  \t0  \t0.0130543  \t0.0496732  \n",
            "248\t298  \t0.0181513  \t0  \t0.0105085  \t0.0496732  \n",
            "249\t297  \t0.0175408  \t0  \t0.011647   \t0.0496732  \n",
            "250\t294  \t0.0175299  \t0  \t0.0127176  \t0.0496732  \n",
            "251\t290  \t0.0174634  \t0  \t0.0128536  \t0.0496732  \n",
            "252\t313  \t0.0180484  \t0  \t0.0100561  \t0.0496732  \n",
            "253\t286  \t0.0184905  \t0  \t0.00981654 \t0.0496732  \n",
            "254\t326  \t0.0167517  \t0  \t0.0106815  \t0.0496732  \n",
            "255\t289  \t0.0169094  \t0  \t0.0091564  \t0.0496732  \n",
            "256\t274  \t0.0174474  \t0  \t0.0114304  \t0.0496732  \n",
            "257\t275  \t0.0190502  \t0  \t0.0112941  \t0.0496732  \n",
            "258\t301  \t0.0178713  \t0  \t0.0121582  \t0.0496732  \n",
            "259\t292  \t0.0178618  \t0  \t0.0129191  \t0.0496732  \n",
            "260\t294  \t0.0173232  \t0  \t0.0123175  \t0.0496732  \n",
            "261\t297  \t0.0167647  \t0  \t0.0120794  \t0.0496732  \n",
            "262\t295  \t0.0164031  \t0  \t0.0114066  \t0.0496732  \n",
            "263\t301  \t0.016595   \t0  \t0.0115489  \t0.0496732  \n",
            "264\t304  \t0.0162186  \t0  \t0.0112879  \t0.0496732  \n",
            "265\t314  \t0.01528    \t0  \t0.010482   \t0.0496732  \n",
            "266\t295  \t0.0154023  \t0  \t0.0108979  \t0.0496732  \n",
            "267\t321  \t0.0140109  \t0  \t0.0093607  \t0.0496732  \n",
            "268\t295  \t0.0145172  \t0  \t0.00990291 \t0.0496732  \n",
            "269\t297  \t0.0149201  \t0  \t0.010369   \t0.0496732  \n",
            "270\t305  \t0.0150322  \t0  \t0.00757146 \t0.0496732  \n",
            "271\t293  \t0.0154264  \t0  \t0.0101984  \t0.0496732  \n",
            "272\t288  \t0.0159122  \t0  \t0.0115926  \t0.0496732  \n",
            "273\t311  \t0.0163069  \t0  \t0.0118533  \t0.0496732  \n",
            "274\t311  \t0.0159463  \t0  \t0.0111044  \t0.0496732  \n",
            "275\t298  \t0.0172641  \t0  \t0.00934239 \t0.0496732  \n",
            "276\t295  \t0.0172606  \t0  \t0.0114898  \t0.0496732  \n",
            "277\t302  \t0.0167977  \t0  \t0.0120347  \t0.0496732  \n",
            "278\t317  \t0.0174012  \t0  \t0.0124965  \t0.0496732  \n",
            "279\t289  \t0.0193114  \t0  \t0.0119447  \t0.0496732  \n",
            "280\t293  \t0.0189915  \t0  \t0.0138131  \t0.0496732  \n",
            "281\t299  \t0.0196999  \t0  \t0.0150259  \t0.0496732  \n",
            "282\t324  \t0.0187452  \t0  \t0.0136024  \t0.0496732  \n",
            "283\t290  \t0.0182576  \t0  \t0.0132129  \t0.0496732  \n",
            "284\t282  \t0.0183762  \t0  \t0.0136979  \t0.0496732  \n",
            "285\t334  \t0.0168453  \t0  \t0.0115632  \t0.0496732  \n",
            "286\t288  \t0.0168121  \t0  \t0.0118779  \t0.0496732  \n",
            "287\t266  \t0.017691   \t0  \t0.0130067  \t0.0496732  \n",
            "288\t309  \t0.0174398  \t0  \t0.0122974  \t0.0496732  \n",
            "289\t294  \t0.0178059  \t0  \t0.0130611  \t0.0496732  \n",
            "290\t305  \t0.0175099  \t0  \t0.0123964  \t0.0496732  \n",
            "291\t274  \t0.0177888  \t0  \t0.0133218  \t0.0496732  \n",
            "292\t278  \t0.0196533  \t0  \t0.0122491  \t0.0496732  \n",
            "293\t318  \t0.0174303  \t0  \t0.00827487 \t0.0496732  \n",
            "294\t293  \t0.0173103  \t0  \t0.0110774  \t0.0496732  \n",
            "295\t296  \t0.0165257  \t0  \t0.0114488  \t0.0496732  \n",
            "296\t313  \t0.0156718  \t0  \t0.0103352  \t0.0496732  \n",
            "297\t285  \t0.0166626  \t0  \t0.0120321  \t0.0496732  \n",
            "298\t279  \t0.0172853  \t0  \t0.0127083  \t0.0496732  \n",
            "299\t317  \t0.0183656  \t0  \t0.0135492  \t0.0496732  \n",
            "300\t320  \t0.0166814  \t0  \t0.0117869  \t0.0496732  \n",
            "301\t306  \t0.0170311  \t0  \t0.0123276  \t0.0496732  \n",
            "302\t304  \t0.0172577  \t0  \t0.0129108  \t0.0496732  \n",
            "303\t306  \t0.0163616  \t0  \t0.012126   \t0.0496732  \n",
            "304\t280  \t0.0155361  \t0  \t0.0116897  \t0.0496732  \n",
            "305\t284  \t0.0163418  \t0  \t0.0123908  \t0.0496732  \n",
            "306\t298  \t0.0167554  \t0  \t0.0124903  \t0.0496732  \n",
            "307\t272  \t0.0178117  \t0  \t0.0135627  \t0.0496732  \n",
            "308\t277  \t0.0186661  \t0  \t0.0115005  \t0.0496732  \n",
            "309\t294  \t0.0178995  \t0  \t0.0127336  \t0.0496732  \n",
            "310\t279  \t0.0195869  \t0  \t0.0122036  \t0.0496732  \n",
            "311\t295  \t0.0192881  \t0  \t0.0110366  \t0.0496732  \n",
            "312\t321  \t0.0175719  \t0  \t0.0119908  \t0.0496732  \n",
            "313\t293  \t0.0175618  \t0  \t0.0130007  \t0.0496732  \n",
            "314\t308  \t0.0174652  \t0  \t0.0129321  \t0.0496732  \n",
            "315\t282  \t0.0178771  \t0  \t0.0134429  \t0.0496732  \n",
            "316\t291  \t0.0185487  \t0  \t0.0143038  \t0.0496732  \n",
            "317\t303  \t0.0183622  \t0  \t0.0137829  \t0.0496732  \n",
            "318\t312  \t0.017109   \t0  \t0.012061   \t0.0496732  \n",
            "319\t300  \t0.0174301  \t0  \t0.0126796  \t0.0496732  \n",
            "320\t301  \t0.0179838  \t0  \t0.00998389 \t0.0496732  \n",
            "321\t307  \t0.0173555  \t0  \t0.00846256 \t0.0496732  \n",
            "322\t271  \t0.0180166  \t0  \t0.0119007  \t0.0496732  \n",
            "323\t277  \t0.0177394  \t0  \t0.0126717  \t0.0496732  \n",
            "324\t298  \t0.0186991  \t0  \t0.0139277  \t0.0496732  \n",
            "325\t323  \t0.0192688  \t0  \t0.0107499  \t0.0496732  \n",
            "326\t313  \t0.0173362  \t0  \t0.0110305  \t0.0496732  \n",
            "327\t303  \t0.0178751  \t0  \t0.0126839  \t0.0496732  \n",
            "328\t310  \t0.0178031  \t0  \t0.0131456  \t0.0496732  \n",
            "329\t294  \t0.018072   \t0  \t0.0137557  \t0.0496732  \n",
            "330\t298  \t0.0183018  \t0  \t0.0142596  \t0.0496732  \n",
            "331\t324  \t0.018445   \t0  \t0.0100659  \t0.0496732  \n",
            "332\t298  \t0.0171784  \t0  \t0.0111178  \t0.0496732  \n",
            "333\t313  \t0.0177338  \t0  \t0.00932748 \t0.0496732  \n",
            "334\t295  \t0.017395   \t0  \t0.0114661  \t0.0496732  \n",
            "335\t277  \t0.0185607  \t0  \t0.010893   \t0.0496732  \n",
            "336\t294  \t0.0187703  \t0  \t0.0136853  \t0.0496732  \n",
            "337\t273  \t0.0192317  \t0  \t0.0147664  \t0.0496732  \n",
            "338\t303  \t0.0182127  \t0  \t0.0133772  \t0.0496732  \n",
            "339\t308  \t0.0191948  \t0  \t0.011191   \t0.0496732  \n",
            "340\t298  \t0.017863   \t0  \t0.0120068  \t0.0496732  \n",
            "341\t315  \t0.0178084  \t0  \t0.00969742 \t0.0496732  \n",
            "342\t299  \t0.0184737  \t0  \t0.00948677 \t0.0496732  \n",
            "343\t305  \t0.0178202  \t0  \t0.0119467  \t0.0496732  \n",
            "344\t292  \t0.0188114  \t0  \t0.0108966  \t0.0496732  \n",
            "345\t314  \t0.0187295  \t0  \t0.00976905 \t0.0496732  \n",
            "346\t320  \t0.0175921  \t0  \t0.0113019  \t0.0496732  \n",
            "347\t300  \t0.0187054  \t0  \t0.0105182  \t0.0496732  \n",
            "348\t312  \t0.0171304  \t0  \t0.00803563 \t0.0496732  \n",
            "349\t308  \t0.0161936  \t0  \t0.00983082 \t0.0496732  \n",
            "350\t281  \t0.018081   \t0  \t0.0102789  \t0.0496732  \n",
            "351\t326  \t0.0170077  \t0  \t0.00861771 \t0.0496732  \n",
            "352\t290  \t0.0159777  \t0  \t0.0100525  \t0.0496732  \n",
            "353\t297  \t0.0151321  \t0  \t0.0107598  \t0.0496732  \n",
            "354\t297  \t0.0165473  \t0  \t0.0121258  \t0.0496732  \n",
            "355\t327  \t0.015689   \t0  \t0.0108599  \t0.0496732  \n",
            "356\t306  \t0.0164928  \t0  \t0.0115724  \t0.0496732  \n",
            "357\t290  \t0.0166983  \t0  \t0.0123764  \t0.0496732  \n",
            "358\t300  \t0.0167818  \t0  \t0.0120586  \t0.0496732  \n",
            "359\t311  \t0.0166693  \t0  \t0.012187   \t0.0496732  \n",
            "360\t308  \t0.0173015  \t0  \t0.00931287 \t0.0496732  \n",
            "361\t306  \t0.0155936  \t0  \t0.0104166  \t0.0496732  \n",
            "362\t315  \t0.0153602  \t0  \t0.0107561  \t0.0496732  \n",
            "363\t274  \t0.0165409  \t0  \t0.0120078  \t0.0496732  \n",
            "364\t289  \t0.0159855  \t0  \t0.0112634  \t0.0496732  \n",
            "365\t298  \t0.0165731  \t0  \t0.00879661 \t0.0496732  \n",
            "366\t312  \t0.016849   \t0  \t0.00824843 \t0.0496732  \n",
            "367\t273  \t0.0178874  \t0  \t0.0116671  \t0.0496732  \n",
            "368\t289  \t0.0183749  \t0  \t0.0132661  \t0.0496732  \n",
            "369\t299  \t0.018419   \t0  \t0.0136591  \t0.0496732  \n",
            "370\t311  \t0.018646   \t0  \t0.0103627  \t0.0496732  \n",
            "371\t305  \t0.017815   \t0  \t0.0117875  \t0.0496732  \n",
            "372\t309  \t0.0182914  \t0  \t0.00998314 \t0.0496732  \n",
            "373\t311  \t0.0180965  \t0  \t0.0122541  \t0.0496732  \n",
            "374\t306  \t0.0176707  \t0  \t0.0127055  \t0.0496732  \n",
            "375\t288  \t0.0183866  \t0  \t0.0137038  \t0.0496732  \n",
            "376\t304  \t0.0190875  \t0  \t0.0109731  \t0.0496732  \n",
            "377\t309  \t0.0170509  \t0  \t0.0114487  \t0.0496732  \n",
            "378\t290  \t0.0171034  \t0  \t0.01208    \t0.0496732  \n",
            "379\t313  \t0.0172138  \t0  \t0.0124685  \t0.0496732  \n",
            "380\t296  \t0.0173545  \t0  \t0.0125849  \t0.0496732  \n",
            "381\t284  \t0.0172895  \t0  \t0.0127028  \t0.0496732  \n",
            "382\t288  \t0.0179574  \t0  \t0.0134778  \t0.0496732  \n",
            "383\t295  \t0.0199602  \t0  \t0.0122835  \t0.0496732  \n",
            "384\t311  \t0.0192838  \t0  \t0.0136871  \t0.0496732  \n",
            "385\t310  \t0.0196749  \t0  \t0.0113964  \t0.0496732  \n",
            "386\t297  \t0.0200821  \t0  \t0.011157   \t0.0496732  \n",
            "387\t291  \t0.0203761  \t0  \t0.0147887  \t0.0496732  \n",
            "388\t308  \t0.01914    \t0  \t0.0141405  \t0.0496732  \n",
            "389\t285  \t0.0198226  \t0  \t0.0159986  \t0.0496732  \n",
            "390\t299  \t0.0199838  \t0  \t0.0166845  \t0.0496732  \n",
            "391\t289  \t0.0198605  \t0  \t0.0161137  \t0.0496732  \n",
            "392\t271  \t0.0195475  \t0  \t0.0155177  \t0.0496732  \n",
            "393\t303  \t0.0194322  \t0  \t0.0156939  \t0.0496732  \n",
            "394\t321  \t0.0185109  \t0  \t0.0134558  \t0.0496732  \n",
            "395\t302  \t0.0188626  \t0  \t0.013811   \t0.0496732  \n",
            "396\t319  \t0.0184304  \t0  \t0.0134022  \t0.0496732  \n",
            "397\t303  \t0.0180476  \t0  \t0.0129791  \t0.0496732  \n",
            "398\t326  \t0.0174495  \t0  \t0.0110269  \t0.0496732  \n",
            "399\t314  \t0.016516   \t0  \t0.0111835  \t0.0496732  \n",
            "400\t291  \t0.0175747  \t0  \t0.00981282 \t0.0496732  \n",
            "401\t297  \t0.0172589  \t0  \t0.011414   \t0.0496732  \n",
            "402\t303  \t0.0162461  \t0  \t0.0108709  \t0.0496732  \n",
            "403\t293  \t0.0161741  \t0  \t0.0102089  \t0.0496732  \n",
            "404\t305  \t0.0166734  \t0  \t0.0112712  \t0.0496732  \n",
            "405\t298  \t0.0159719  \t0  \t0.010917   \t0.0496732  \n",
            "406\t300  \t0.0170103  \t0  \t0.0119368  \t0.0496732  \n",
            "407\t285  \t0.0161469  \t0  \t0.0114141  \t0.0496732  \n",
            "408\t288  \t0.0169605  \t0  \t0.0124965  \t0.0496732  \n",
            "409\t300  \t0.0182058  \t0  \t0.0106187  \t0.0496732  \n",
            "410\t293  \t0.0182999  \t0  \t0.0121212  \t0.0496732  \n",
            "411\t295  \t0.0174062  \t0  \t0.012477   \t0.0496732  \n",
            "412\t303  \t0.0168304  \t0  \t0.0118972  \t0.0496732  \n",
            "413\t288  \t0.0181609  \t0  \t0.0140337  \t0.0496732  \n",
            "414\t299  \t0.0189351  \t0  \t0.0133767  \t0.0496732  \n",
            "415\t295  \t0.01927    \t0  \t0.0115031  \t0.0496732  \n",
            "416\t298  \t0.0184887  \t0  \t0.0130731  \t0.0496732  \n",
            "417\t274  \t0.01936    \t0  \t0.0147415  \t0.0496732  \n",
            "418\t294  \t0.0186782  \t0  \t0.0147892  \t0.0496732  \n",
            "419\t310  \t0.0179136  \t0  \t0.0131617  \t0.0496732  \n",
            "420\t323  \t0.0173658  \t0  \t0.0123769  \t0.0496732  \n",
            "421\t290  \t0.0170378  \t0  \t0.0126981  \t0.0496732  \n",
            "422\t312  \t0.0172719  \t0  \t0.0126009  \t0.0496732  \n",
            "423\t312  \t0.0166296  \t0  \t0.0119175  \t0.0496732  \n",
            "424\t300  \t0.0165409  \t0  \t0.0117429  \t0.0496732  \n",
            "425\t307  \t0.0157002  \t0  \t0.0111212  \t0.0496732  \n",
            "426\t319  \t0.0156133  \t0  \t0.0111771  \t0.0496732  \n",
            "427\t298  \t0.0158187  \t0  \t0.0116103  \t0.0496732  \n",
            "428\t310  \t0.0156464  \t0  \t0.0109536  \t0.0496732  \n",
            "429\t282  \t0.0162198  \t0  \t0.0120118  \t0.0496732  \n",
            "430\t299  \t0.0165247  \t0  \t0.0121836  \t0.0496732  \n",
            "431\t318  \t0.0171404  \t0  \t0.00900724 \t0.0496732  \n",
            "432\t315  \t0.0170027  \t0  \t0.00814484 \t0.0496732  \n",
            "433\t295  \t0.0153179  \t0  \t0.00892079 \t0.0496732  \n",
            "434\t282  \t0.01517    \t0  \t0.0104222  \t0.0496732  \n",
            "435\t283  \t0.0162035  \t0  \t0.00872586 \t0.0496732  \n",
            "436\t292  \t0.0158776  \t0  \t0.0108001  \t0.0496732  \n",
            "437\t309  \t0.0143314  \t0  \t0.00996929 \t0.0496732  \n",
            "438\t295  \t0.0150332  \t0  \t0.0104991  \t0.0496732  \n",
            "439\t306  \t0.0149602  \t0  \t0.00929274 \t0.0496732  \n",
            "440\t320  \t0.0152678  \t0  \t0.0100975  \t0.0496732  \n",
            "441\t292  \t0.0174199  \t0  \t0.00956098 \t0.0496732  \n",
            "442\t293  \t0.0174072  \t0  \t0.0117206  \t0.0496732  \n",
            "443\t311  \t0.0168943  \t0  \t0.0117211  \t0.0496732  \n",
            "444\t327  \t0.016849   \t0  \t0.0091191  \t0.0496732  \n",
            "445\t301  \t0.0168236  \t0  \t0.00808952 \t0.0496732  \n",
            "446\t300  \t0.0167575  \t0  \t0.0106969  \t0.0496732  \n",
            "447\t287  \t0.0166084  \t0  \t0.0115667  \t0.0496732  \n",
            "448\t306  \t0.0171753  \t0  \t0.0121648  \t0.0496732  \n",
            "449\t293  \t0.0178967  \t0  \t0.0131277  \t0.0496732  \n",
            "450\t301  \t0.0181631  \t0  \t0.0135339  \t0.0496732  \n",
            "451\t278  \t0.0185582  \t0  \t0.0139934  \t0.0496732  \n",
            "452\t299  \t0.0193857  \t0  \t0.0148699  \t0.0496732  \n",
            "453\t309  \t0.0194627  \t0  \t0.0113551  \t0.0496732  \n",
            "454\t310  \t0.0185505  \t0  \t0.0124129  \t0.0496732  \n",
            "455\t314  \t0.0185961  \t0  \t0.0135377  \t0.0496732  \n",
            "456\t278  \t0.0189962  \t0  \t0.0146174  \t0.0496732  \n",
            "457\t329  \t0.0175623  \t0  \t0.0123827  \t0.0496732  \n",
            "458\t302  \t0.0177313  \t0  \t0.012758   \t0.0496732  \n",
            "459\t303  \t0.0175484  \t0  \t0.011683   \t0.0496732  \n",
            "460\t316  \t0.0167821  \t0  \t0.0114048  \t0.0496732  \n",
            "461\t310  \t0.0172525  \t0  \t0.0122634  \t0.0496732  \n",
            "462\t299  \t0.0167041  \t0  \t0.0119942  \t0.0496732  \n",
            "463\t289  \t0.0179093  \t0  \t0.0136871  \t0.0496732  \n",
            "464\t312  \t0.0193561  \t0  \t0.0114147  \t0.0496732  \n",
            "465\t300  \t0.0183268  \t0  \t0.0127568  \t0.0496732  \n",
            "466\t281  \t0.0182943  \t0  \t0.013702   \t0.0496732  \n",
            "467\t298  \t0.0176451  \t0  \t0.0126815  \t0.0496732  \n",
            "468\t262  \t0.0197976  \t0  \t0.0126409  \t0.0496732  \n",
            "469\t300  \t0.0189733  \t0  \t0.0118213  \t0.0496732  \n",
            "470\t283  \t0.0185906  \t0  \t0.0100636  \t0.0496732  \n",
            "471\t294  \t0.0177698  \t0  \t0.0116322  \t0.0496732  \n",
            "472\t269  \t0.0180719  \t0  \t0.013435   \t0.0496732  \n",
            "473\t300  \t0.0183544  \t0  \t0.0104532  \t0.0496732  \n",
            "474\t300  \t0.0182895  \t0  \t0.00927914 \t0.0496732  \n",
            "475\t305  \t0.0177935  \t0  \t0.0085904  \t0.0496732  \n",
            "476\t300  \t0.0181828  \t0  \t0.00883128 \t0.0496732  \n",
            "477\t291  \t0.0177013  \t0  \t0.0113483  \t0.0496732  \n",
            "478\t296  \t0.0170173  \t0  \t0.0120834  \t0.0496732  \n",
            "479\t292  \t0.0169834  \t0  \t0.0120547  \t0.0496732  \n",
            "480\t319  \t0.0159229  \t0  \t0.0107635  \t0.0496732  \n",
            "481\t299  \t0.0164757  \t0  \t0.0115469  \t0.0496732  \n",
            "482\t320  \t0.016154   \t0  \t0.0111949  \t0.0496732  \n",
            "483\t316  \t0.0170873  \t0  \t0.0121642  \t0.0496732  \n",
            "484\t293  \t0.0167099  \t0  \t0.0118322  \t0.0496732  \n",
            "485\t291  \t0.0168759  \t0  \t0.012014   \t0.0496732  \n",
            "486\t307  \t0.0171385  \t0  \t0.0122839  \t0.0496732  \n",
            "487\t316  \t0.0170528  \t0  \t0.0119135  \t0.0496732  \n",
            "488\t285  \t0.017227   \t0  \t0.0125806  \t0.0496732  \n",
            "489\t302  \t0.0166879  \t0  \t0.0115921  \t0.0496732  \n",
            "490\t289  \t0.0168337  \t0  \t0.0118123  \t0.0496732  \n",
            "491\t286  \t0.0172585  \t0  \t0.0125459  \t0.0496732  \n",
            "492\t325  \t0.0166542  \t0  \t0.0119069  \t0.0496732  \n",
            "493\t305  \t0.017097   \t0  \t0.0121028  \t0.0496732  \n",
            "494\t272  \t0.0179686  \t0  \t0.0134165  \t0.0496732  \n",
            "495\t292  \t0.0190235  \t0  \t0.0111505  \t0.0496732  \n",
            "496\t320  \t0.0169469  \t0  \t0.0110421  \t0.0496732  \n",
            "497\t290  \t0.017619   \t0  \t0.011222   \t0.0496732  \n",
            "498\t292  \t0.0162781  \t0  \t0.0111411  \t0.0496732  \n",
            "499\t304  \t0.0168937  \t0  \t0.0119963  \t0.0496732  \n",
            "Best individual :  sin(ARG260) (0.04967320261437908,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc-wqAxoT45D"
      },
      "source": [
        "**Prediction task**\n",
        "\n",
        "1.   Create new data from the list of fittest individual (fittest features) for both training and testing data.\n",
        "2.   Fit the svm with transformed training data\n",
        "3.   Predict the transformed testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRyeLuWSrGwr",
        "outputId": "e405f097-efbe-4814-b94d-fef4b60195ef"
      },
      "source": [
        "#1. Evaluate the expression for training set\n",
        "list_train_vecs = []\n",
        "for individual in hof:\n",
        "  if individual.fitness.values[0] > 0:\n",
        "      func = toolbox.compile(expr=individual)\n",
        "      vec = []\n",
        "      for x_train in X_train: #Iterate every vector x (row) in data (matrix) X\n",
        "        try:\n",
        "          val = func(*x_train)\n",
        "          vec.append(val)\n",
        "        except:\n",
        "          vec.append(0)\n",
        "      list_train_vecs.append(vec)\n",
        "\n",
        "#2. Evaluate the expression for testing set\n",
        "list_test_vecs = []\n",
        "for individual in hof:\n",
        "  if individual.fitness.values[0] > 0:\n",
        "      func = toolbox.compile(expr=individual)\n",
        "      vec = []\n",
        "      for x_test in X_test: #Iterate every vector x (row) in data (matrix) X\n",
        "        try:\n",
        "          val = func(*x_test)\n",
        "          vec.append(val)\n",
        "        except:\n",
        "          vec.append(0)\n",
        "      list_test_vecs.append(vec)\n",
        "\n",
        "#2. Convert list_vecs to numpy array\n",
        "X_train_new = np.array(list_train_vecs).T   #Need to refactor X_train g\n",
        "X_train_new = np.nan_to_num(X_train_new, copy=True, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "X_train_new = np.hstack((X_train, X_train_new))\n",
        "\n",
        "X_test_new = np.array(list_test_vecs).T   #Need to refactor X_test g\n",
        "X_test_new = np.nan_to_num(X_test_new, copy=True, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "X_test_new = np.hstack((X_test, X_test_new))\n",
        "\n",
        "\n",
        "#3. Fit the SVM\n",
        "list_accuracy_result = []\n",
        "list_f1_result = []\n",
        "for c in range(-5,6):\n",
        "  for g in range(-4,6):\n",
        "    clf_svc = SVC(C=2**c, gamma=2**g)\n",
        "    clf_svc.fit(X_train, y_train)\n",
        "    y_pred = clf_svc.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    list_accuracy_result.append(accuracy)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    list_f1_result.append(f1)\n",
        "\n",
        "print(\"SVM - Highest Accuracy-Score: \", max(list_accuracy_result))\n",
        "print(\"SVM - Highest F1-Score: \", max(list_f1_result))\n",
        "\n",
        "\n",
        "#4. Fit the SVM (SVGPM)\n",
        "list_accuracy_result = []\n",
        "list_f1_result = []\n",
        "for c in range(-5,6):\n",
        "  for g in range(-4,6):\n",
        "    clf_svc = SVC(C=2**c, gamma=2**g)\n",
        "    clf_svc.fit(X_train_new, y_train)\n",
        "    y_pred = clf_svc.predict(X_test_new)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    list_accuracy_result.append(accuracy)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    list_f1_result.append(f1)\n",
        "\n",
        "print(\"SVGPM V2 - Highest Accuracy-Score: \", max(list_accuracy_result))\n",
        "print(\"SVGPM V2 - Highest F1-Score: \", max(list_f1_result))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM - Highest Accuracy-Score:  0.6233766233766234\n",
            "SVM - Highest F1-Score:  0.6133333333333334\n",
            "SVGPM V2 - Highest Accuracy-Score:  0.5714285714285714\n",
            "SVGPM V2 - Highest F1-Score:  0.66\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}